{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "403ea9ab-4804-4ed2-ae3a-e0d32857e3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import expr, explode, col\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from osgeo import gdal, gdalconst\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57543734-4449-41ce-ab82-4175973cad05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "catalog_name = \"geospatial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522ce534-bae3-42f2-8dfa-61fc55f82499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"access_key\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"secret_key\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'eu-west-2'      # Match your bucket region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a34ae5-39d5-40f6-9c70-3115ca5f2b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = SedonaContext.builder(). \\\n",
    "    config(\"spark.hadoop.fs.s3a.bucket.wherobots-examples.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b4075c-dd03-4763-b3b3-eaa3f4c1326d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure GDAL\n",
    "gdal.UseExceptions()\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN', 'YES')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Initialize and return boto3 S3 client with credentials\"\"\"\n",
    "    return boto3.client('s3',\n",
    "                       aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                       aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                       region_name=os.getenv('AWS_DEFAULT_REGION', 'eu-west-2'))\n",
    "\n",
    "def upload_to_s3(local_path, s3_path):\n",
    "    \"\"\"Upload file to S3 using boto3\"\"\"\n",
    "    s3 = get_s3_client()\n",
    "    try:\n",
    "        bucket, key = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        s3.upload_file(local_path, bucket, key)\n",
    "        logger.info(f\"Successfully uploaded to {s3_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Failed to upload to S3: {e}\")\n",
    "        return False\n",
    "\n",
    "def merge_tiffs(binary_df, output_path):\n",
    "    \"\"\"\n",
    "    Merges multiple TIFF binary data from a Spark DataFrame into a single TIFF using GDAL\n",
    "    \n",
    "    Args:\n",
    "        binary_df: PySpark DataFrame containing binary TIFF data\n",
    "        output_path: Path to save the merged TIFF file (can be local or S3 path)\n",
    "    \"\"\"\n",
    "    # Collect all TIFF binary data to driver\n",
    "    tiff_data = binary_df.select(col(\"raster_binary\")).collect()\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_files = []\n",
    "        \n",
    "        # Write each binary TIFF to a temporary file\n",
    "        for i, row in enumerate(tiff_data):\n",
    "            temp_file = os.path.join(temp_dir, f\"temp_{i}.tif\")\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                f.write(row['raster_binary'])\n",
    "            temp_files.append(temp_file)\n",
    "        \n",
    "        if len(temp_files) > 0:\n",
    "            try:\n",
    "                # Create VRT mosaic\n",
    "                vrt_file = os.path.join(temp_dir, \"mosaic.vrt\")\n",
    "                gdal.BuildVRT(vrt_file, temp_files)\n",
    "                \n",
    "                # Create local output file\n",
    "                local_output = os.path.join(temp_dir, \"merged_output.tif\")\n",
    "                translate_options = gdal.TranslateOptions(\n",
    "                    creationOptions=['COMPRESS=DEFLATE', 'TILED=YES', 'BIGTIFF=IF_SAFER']\n",
    "                )\n",
    "                gdal.Translate(local_output, vrt_file, options=translate_options)\n",
    "                \n",
    "                # Handle output destination\n",
    "                if output_path.startswith('s3://'):\n",
    "                    if upload_to_s3(local_output, output_path):\n",
    "                        logger.info(f\"Merged {len(temp_files)} TIFFs to {output_path}\")\n",
    "                    else:\n",
    "                        raise Exception(\"Failed to upload to S3\")\n",
    "                else:\n",
    "                    os.rename(local_output, output_path)\n",
    "                    logger.info(f\"Merged {len(temp_files)} TIFFs to {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during merge: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.warning(\"No TIFF files found to merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc1f69e-9053-4df0-b240-dc670b3aa655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "classification_df = spark.table(\"geospatial.soma.classification\").withColumn(\"tile\", expr(\"RS_FromGeoTiff(raster_binary)\")).repartitionByRange(20,\"rn\")\n",
    "classification_df = classification_df.withColumn(\"maxValue\", expr(\"\"\"RS_SummaryStats(tile, \"max\", 1, false)\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a51d2d9-c8a7-4737-af01-caef90b52cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "no_interpolation = classification_df.filter(classification_df[\"maxValue\"] != 999).select(\"tile_x\", \"tile_y\",\"rn\", \"year\", \"index\", \"raster_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5efa229-0773-4e68-859e-0869eddbf5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "interpolated_df = classification_df.filter(classification_df[\"maxValue\"] == 999).select(\"tile_x\", \"tile_y\",\"rn\", \"year\", \"index\", expr(\"RS_Interpolate(tile, 2.0, 'variable', 48.0, 6.0)\").alias(\"tile\")).withColumn(\"raster_binary\", expr(\"RS_AsGeoTiff(tile)\")).select(\"tile_x\", \"tile_y\",\"rn\", \"year\", \"index\", \"raster_binary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d362d07-037a-45c4-86b8-caa85f16720a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_df = interpolated_df.unionByName(no_interpolation, allowMissingColumns=False)\n",
    "union_df.write.mode(\"overwrite\").saveAsTable(\"geospatial.soma.interpolation\")\n",
    "union_df = spark.table(\"geospatial.soma.interpolation\").withColumn(\"tile\", expr(\"RS_FromGeoTiff(raster_binary)\")).repartitionByRange(10,\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d761cec-04ea-45ee-a513-51d2f4d8d59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_tiff_2025 = f\"s3://{dataset_bucket_name}/geospatial-dataset/raster/orthophoto/soma/interpolation/interpolation_2025.tif\"\n",
    "output_tiff_2022 = f\"s3://{dataset_bucket_name}/geospatial-dataset/raster/orthophoto/soma/interpolation/interpolation_2022.tif\"\n",
    "\n",
    "union_df_2025 = union_df.filter(union_df[\"year\"] == 2025)\n",
    "union_df_2022 = union_df.filter(union_df[\"year\"] == 2022)\n",
    "merge_tiffs(union_df_2025, output_tiff_2025)\n",
    "merge_tiffs(union_df_2022, output_tiff_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaeb6a5-6808-4d00-b319-9687eaa3b240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_df.createOrReplaceTempView(\"union_df_vw\")\n",
    "\n",
    "merged_raster = sedona.sql(\"\"\"\n",
    "    SELECT rn, RS_Union_Aggr(tile, index) AS raster\n",
    "    FROM union_df_vw\n",
    "    GROUP BY rn\n",
    "\"\"\").repartitionByRange(10,\"rn\")\n",
    "\n",
    "merged_raster.createOrReplaceTempView(\"merged_raster_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a8e663-8732-484e-9ab5-789a1632860f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diff_raster = merged_raster.withColumn(\"diff_band\", expr( \n",
    "        \"RS_LogicalDifference(\"\n",
    "        \"RS_BandAsArray(raster, 1), RS_BandAsArray(raster, 2)\"\n",
    "        \")\")).repartitionByRange(10,\"rn\")\n",
    "\n",
    "result_df = diff_raster.select(\"rn\", expr(\"RS_AddBandFromArray(raster, diff_band) AS raster\").alias(\"raster\")).withColumn(\"raster_binary\", expr(\"RS_AsGeoTiff(raster)\")).repartitionByRange(10,\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcdc18f1-4062-4b16-8199-5c75358ad3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.select(\"rn\", \"raster_binary\").write.mode(\"overwrite\").saveAsTable(\"geospatial.soma.change_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94a1694-e5ac-475f-9ac5-022e47adc3c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge TIFFs\n",
    "result_df = spark.table(\"geospatial.soma.change_detection\").withColumn(\"tile\", expr(\"RS_FromGeoTiff(raster_binary)\")).repartitionByRange(10,\"rn\")\n",
    "output_tiff = f\"s3://{dataset_bucket_name}/geospatial-dataset/raster/orthophoto/soma/difference/difference_output.tif\"\n",
    "merge_tiffs(result_df, output_tiff)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4248342033090984,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06-raster-change-detection-interpolation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
