{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c8f473-0d2e-4771-a521-e56396a50cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from osgeo import gdal, gdalconst\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9cee58-0ce8-4c27-a041-c14beea1c761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"access_key\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"secret_key\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'eu-west-2'      # Match your bucket region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769c2604-0490-4a19-8ae0-46c991adeea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import logging\n",
    "import tempfile\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Don't call UseExceptions() immediately - defer it\n",
    "# gdal.UseExceptions()  # Comment this out\n",
    "\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN', 'YES')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_s3_client():\n",
    "    \"\"\"Initialize and return boto3 S3 client with credentials\"\"\"\n",
    "    return boto3.client('s3',\n",
    "                       aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                       aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                       region_name=os.getenv('AWS_DEFAULT_REGION', 'eu-west-2'))\n",
    "\n",
    "def upload_to_s3(local_path, s3_path):\n",
    "    \"\"\"Upload file to S3 using boto3\"\"\"\n",
    "    s3 = get_s3_client()\n",
    "    try:\n",
    "        bucket, key = s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        s3.upload_file(local_path, bucket, key)\n",
    "        logger.info(f\"Successfully uploaded to {s3_path}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Failed to upload to S3: {e}\")\n",
    "        return False\n",
    "\n",
    "def merge_tiffs(binary_df, output_path):\n",
    "    \"\"\"\n",
    "    Merges multiple TIFF binary data from a Spark DataFrame into a single TIFF using GDAL\n",
    "    \n",
    "    Args:\n",
    "        binary_df: PySpark DataFrame containing binary TIFF data\n",
    "        output_path: Path to save the merged TIFF file (can be local or S3 path)\n",
    "    \"\"\"\n",
    "    # Collect all TIFF binary data to driver\n",
    "    tiff_data = binary_df.select(col(\"raster_binary\")).collect()\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_files = []\n",
    "        \n",
    "        # Write each binary TIFF to a temporary file\n",
    "        for i, row in enumerate(tiff_data):\n",
    "            temp_file = os.path.join(temp_dir, f\"temp_{i}.tif\")\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                f.write(row['raster_binary'])\n",
    "            temp_files.append(temp_file)\n",
    "        \n",
    "        if len(temp_files) > 0:\n",
    "            try:\n",
    "                # Enable exceptions here if needed, after imports are done\n",
    "                # gdal.UseExceptions()\n",
    "                \n",
    "                # Create VRT mosaic\n",
    "                vrt_file = os.path.join(temp_dir, \"mosaic.vrt\")\n",
    "                vrt_ds = gdal.BuildVRT(vrt_file, temp_files)\n",
    "                if vrt_ds is None:\n",
    "                    raise Exception(f\"Failed to create VRT: {gdal.GetLastErrorMsg()}\")\n",
    "                vrt_ds = None  # Close the dataset\n",
    "                \n",
    "                # Create local output file\n",
    "                local_output = os.path.join(temp_dir, \"merged_output.tif\")\n",
    "                translate_options = gdal.TranslateOptions(\n",
    "                    creationOptions=['COMPRESS=DEFLATE', 'TILED=YES', 'BIGTIFF=IF_SAFER']\n",
    "                )\n",
    "                ds = gdal.Translate(local_output, vrt_file, options=translate_options)\n",
    "                if ds is None:\n",
    "                    raise Exception(f\"Failed to translate: {gdal.GetLastErrorMsg()}\")\n",
    "                ds = None  # Close the dataset\n",
    "                \n",
    "                # Handle output destination\n",
    "                if output_path.startswith('s3://'):\n",
    "                    if upload_to_s3(local_output, output_path):\n",
    "                        logger.info(f\"Merged {len(temp_files)} TIFFs to {output_path}\")\n",
    "                    else:\n",
    "                        raise Exception(\"Failed to upload to S3\")\n",
    "                else:\n",
    "                    os.rename(local_output, output_path)\n",
    "                    logger.info(f\"Merged {len(temp_files)} TIFFs to {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during merge: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.warning(\"No TIFF files found to merge\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "merge_images",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
