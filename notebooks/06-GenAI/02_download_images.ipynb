{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b80ee95-baee-4958-8612-0482df071dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../get_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d920db-648c-4cde-babd-9592b25f0a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting the current user\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = get_username_from_email(user_email)\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c94c68-571f-447b-841b-eed88014dea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "catalog_name = \"geospatial\"\n",
    "schema_name = \"zoetermeer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f92d2ec-bddb-4806-bd70-91439e76ed2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.monumenten_{username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62cb2c5-5f04-46e6-a175-2a25bb43045e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "\n",
    "# Base path to Unity Catalog volume\n",
    "SOURCE_TABLE = f\"{catalog_name}.{schema_name}.monumenten_{username}\"\n",
    "VOLUME_PATH = f\"/Volumes/{catalog_name}/{schema_name}/monumenten_{username}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d52720-ffbc-4f31-9c57-6e0462690de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import urllib3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Disable SSL warnings since we're bypassing verification\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def download_image(pic_url, fid):\n",
    "    \"\"\"Download image from URL and save to volume\"\"\"\n",
    "    try:\n",
    "        if not pic_url or not fid:\n",
    "            return f\"Missing URL or FID\"\n",
    "        \n",
    "        # Get filename from URL\n",
    "        parsed_url = urlparse(pic_url)\n",
    "        filename = os.path.basename(parsed_url.path)\n",
    "        if not filename or '.' not in filename:\n",
    "            filename = f\"image_{hash(pic_url) % 10000}.jpg\"\n",
    "        \n",
    "        # Create folder for this fid\n",
    "        folder_path = os.path.join(VOLUME_PATH, str(fid))\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        # Full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Skip if already exists\n",
    "        if os.path.exists(file_path):\n",
    "            return f\"Already exists: {filename}\"\n",
    "        \n",
    "        # Download image with SSL verification disabled\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (compatible)'}\n",
    "        response = requests.get(pic_url, headers=headers, timeout=30, verify=False)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save file\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        return f\"Downloaded: {filename}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Main execution\n",
    "print(\"Reading table...\")\n",
    "df = spark.table(SOURCE_TABLE)\n",
    "\n",
    "print(f\"Table columns: {df.columns}\")\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "df.select(\"pic_url\", \"fid\").show(5, truncate=False)\n",
    "\n",
    "# Get all records with pic_url and fid\n",
    "records = df.select(\"pic_url\", \"fid\").filter(\n",
    "    col(\"pic_url\").isNotNull() & \n",
    "    col(\"fid\").isNotNull()\n",
    ").collect()\n",
    "\n",
    "print(f\"\\nFound {len(records)} records to process\")\n",
    "\n",
    "# Ensure volume directory exists\n",
    "os.makedirs(VOLUME_PATH, exist_ok=True)\n",
    "\n",
    "# Create a session for connection reuse\n",
    "session = requests.Session()\n",
    "session.verify = False\n",
    "session.headers.update({'User-Agent': 'Mozilla/5.0 (compatible)'})\n",
    "\n",
    "def download_single_image(args):\n",
    "    \"\"\"Download a single image - designed for parallel execution\"\"\"\n",
    "    row, index, total = args\n",
    "    pic_url, fid = row.pic_url, row.fid\n",
    "    \n",
    "    try:\n",
    "        if not pic_url or not fid:\n",
    "            return index, f\"Missing URL or FID\"\n",
    "        \n",
    "        # Get filename from URL\n",
    "        parsed_url = urlparse(pic_url)\n",
    "        filename = os.path.basename(parsed_url.path)\n",
    "        if not filename or '.' not in filename:\n",
    "            filename = f\"image_{hash(pic_url) % 10000}.jpg\"\n",
    "        \n",
    "        # Create folder for this fid\n",
    "        folder_path = os.path.join(VOLUME_PATH, str(fid))\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        # Full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Skip if already exists\n",
    "        if os.path.exists(file_path):\n",
    "            return index, f\"Already exists: {filename}\"\n",
    "        \n",
    "        # Download image using session\n",
    "        response = session.get(pic_url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save file\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        return index, f\"Downloaded: {filename}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return index, f\"Error: {str(e)}\"\n",
    "\n",
    "# Process images with parallel downloads\n",
    "print(\"Starting parallel downloads...\")\n",
    "start_time = time.time()\n",
    "\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel downloads\n",
    "max_workers = min(10, len(records))  # Max 10 concurrent downloads\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all download tasks\n",
    "    future_to_args = {\n",
    "        executor.submit(download_single_image, (row, i, len(records))): (row, i) \n",
    "        for i, row in enumerate(records, 1)\n",
    "    }\n",
    "    \n",
    "    # Process completed downloads\n",
    "    for future in as_completed(future_to_args):\n",
    "        row, i = future_to_args[future]\n",
    "        try:\n",
    "            index, result = future.result()\n",
    "            \n",
    "            if result.startswith(\"Downloaded\") or result.startswith(\"Already exists\"):\n",
    "                success_count += 1\n",
    "                print(f\"✓ {i}/{len(records)}: FID={row.fid} - {result}\")\n",
    "            else:\n",
    "                error_count += 1\n",
    "                print(f\"✗ {i}/{len(records)}: FID={row.fid} - {result}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"✗ {i}/{len(records)}: FID={row.fid} - Unexpected error: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\nCompleted in {duration:.1f} seconds!\")\n",
    "print(f\"Success: {success_count}\")\n",
    "print(f\"Errors: {error_count}\")\n",
    "print(f\"Average time per image: {duration/len(records):.2f} seconds\")\n",
    "print(f\"Files saved to: {VOLUME_PATH}\")\n",
    "\n",
    "# Close the session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227fa9f3-4b08-45c6-93ee-2c5b62b63d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5324906299978246,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_download_images",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
