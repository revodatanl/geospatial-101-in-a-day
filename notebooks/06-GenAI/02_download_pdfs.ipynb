{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50d6546-6f71-4e04-9028-ef9093db2829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4==4.14.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b80ee95-baee-4958-8612-0482df071dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../get_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d920db-648c-4cde-babd-9592b25f0a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting the current user\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = get_username_from_email(user_email)\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c94c68-571f-447b-841b-eed88014dea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "catalog_name = \"geospatial\"\n",
    "schema_name = \"zoetermeer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "981bd19c-c4b6-4b09-b524-acab5e5ac831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP VOLUME IF EXISTS {catalog_name}.{schema_name}.vaststellingsbesluit_{username}\")\n",
    "spark.sql(f\"DROP VOLUME IF EXISTS {catalog_name}.{schema_name}.verwijzing_{username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f92d2ec-bddb-4806-bd70-91439e76ed2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.vaststellingsbesluit_{username}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.verwijzing_{username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d37025-773a-4e58-bb63-a48aa68fd7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "\n",
    "class PDFDownloader:\n",
    "    \"\"\"Downloads PDFs from landing pages listed in a Spark DataFrame.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, volume_path: str, url_column: str = \"base_url\"):\n",
    "        self.df = df                           # ðŸ”¹ DataFrame now stored here\n",
    "        self.volume_path = volume_path\n",
    "        self.url_column = url_column\n",
    "        self.session = self._create_session()\n",
    "        \n",
    "    def _create_session(self) -> requests.Session:\n",
    "        session = requests.Session()\n",
    "        session.verify = False\n",
    "        session.headers.update({\"User-Agent\": \"Mozilla/5.0 (compatible)\"})\n",
    "        return session\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Fetch landing page & parse PDF links\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _fetch_pdf_links_from_landing_page(self, landing_url: str) -> List[str]:\n",
    "        \"\"\"Download HTML and extract all PDF links.\"\"\"\n",
    "        try:\n",
    "            resp = self.session.get(landing_url, timeout=20)\n",
    "            resp.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching landing page {landing_url}: {e}\")\n",
    "            return []\n",
    "\n",
    "        return self._extract_pdf_urls_from_html(resp.text, landing_url)\n",
    "\n",
    "    def _extract_pdf_urls_from_html(self, html: str, base_url: str) -> List[str]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        pdf_urls = []\n",
    "\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"].strip()\n",
    "            if href.lower().endswith(\".pdf\"):\n",
    "                full = urljoin(base_url, href)\n",
    "                pdf_urls.append(full)\n",
    "\n",
    "        return list(set(pdf_urls))  # distinct\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    def _get_filename(self, pdf_url: str) -> str:\n",
    "        filename = os.path.basename(urlparse(pdf_url).path)\n",
    "        if not filename or not filename.lower().endswith(\".pdf\"):\n",
    "            filename = f\"doc_{hash(pdf_url) % 100000}.pdf\"\n",
    "        return filename\n",
    "\n",
    "    def _download_pdf(self, pdf_url: str, fid: str):\n",
    "        filename = self._get_filename(pdf_url)\n",
    "        folder = os.path.join(self.volume_path, str(fid))\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        file_path = os.path.join(folder, filename)\n",
    "\n",
    "        # Skip if file already exists\n",
    "        if os.path.exists(file_path):\n",
    "            return True, f\"Already exists: {filename}\"\n",
    "\n",
    "        try:\n",
    "            resp = self.session.get(pdf_url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "\n",
    "            return True, f\"Downloaded: {filename}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error downloading {filename}: {str(e)}\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Process one landing page â†’ extract all PDFs â†’ download them\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _process_record(self, args: Tuple):\n",
    "        row, index, total = args\n",
    "        landing_page_url = getattr(row, self.url_column)\n",
    "        fid = row.fid\n",
    "\n",
    "        if not landing_page_url:\n",
    "            return [(fid, False, \"Missing landing page URL\")]\n",
    "\n",
    "        pdf_urls = self._fetch_pdf_links_from_landing_page(landing_page_url)\n",
    "\n",
    "        if not pdf_urls:\n",
    "            return [(fid, False, \"No PDFs found on landing page\")]\n",
    "\n",
    "        results = []\n",
    "        for pdf_url in pdf_urls:\n",
    "            success, msg = self._download_pdf(pdf_url, fid)\n",
    "            results.append((fid, success, msg))\n",
    "            print(f\"{index}/{total} - FID={fid}: {msg}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    def run(self, max_workers: int = 10):\n",
    "        # Collect rows from given DataFrame\n",
    "        records = self.df.select(self.url_column, \"fid\") \\\n",
    "                         .filter(self.df[self.url_column].isNotNull()) \\\n",
    "                         .filter(self.df.fid.isNotNull()) \\\n",
    "                         .collect()\n",
    "\n",
    "        print(f\"Found {len(records)} landing pages\")\n",
    "\n",
    "        if not records:\n",
    "            return\n",
    "\n",
    "        os.makedirs(self.volume_path, exist_ok=True)\n",
    "\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        start = time.time()\n",
    "\n",
    "        workers = min(max_workers, len(records))\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = [\n",
    "                executor.submit(self._process_record, (row, i, len(records)))\n",
    "                for i, row in enumerate(records, start=1)\n",
    "            ]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    for fid, ok, msg in future.result():\n",
    "                        if ok:\n",
    "                            success_count += 1\n",
    "                        else:\n",
    "                            error_count += 1\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    print(\"Unexpected error:\", e)\n",
    "\n",
    "        print(\"\\n========== SUMMARY ==========\")\n",
    "        print(\"Successful downloads:\", success_count)\n",
    "        print(\"Failed downloads:\", error_count)\n",
    "        print(\"Output folder:\", self.volume_path)\n",
    "        print(\"Total time:\", round(time.time() - start, 1), \"seconds\")\n",
    "        print(\"=============================\")\n",
    "\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571abf34-c27a-426a-8a04-702b42da335d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"base_url\":1500,\"verwijzingNaarTekst\":280},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"verwijzingNaarTekst\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}},\"base_url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1764766992346}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, regexp_replace, expr, col, array_distinct, element_at\n",
    "\n",
    "SOURCE_TABLE: str = f\"{catalog_name}.{schema_name}.bestemmingsplangebied_{username}\"\n",
    "\n",
    "df = spark.read.table(SOURCE_TABLE)\n",
    "\n",
    "df_with_base_url = df.withColumn(\n",
    "    \"base_url\",\n",
    "    element_at(array_distinct(                           # remove duplicates\n",
    "        expr(\"\"\"\n",
    "            transform(\n",
    "                split(verwijzingNaarTekst, ','),   -- split into list\n",
    "                x -> regexp_replace(x, '/[^/]+$', '/')  -- remove last segment (the HTML file)\n",
    "            )\n",
    "        \"\"\")\n",
    "    ),1\n",
    "))\n",
    "\n",
    "display(df_with_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2685f15b-2721-4905-9243-d798a0f514bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_PATH = f\"/Volumes/{catalog_name}/{schema_name}/verwijzing_{username}\"\n",
    "main(df_with_base_url, VOLUME_PATH, \"base_url\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5324906299978246,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_download_pdfs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
