{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7121876a-660c-4c6f-8dd0-2fa72a49c208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Goal\n",
    "\n",
    "The objective of this notebook is to ingest GeoPackage data from a storage account (Azure), S3 bucket (AWS), or Unity Catalog volume, and load it into Unity Catalog tables using Apache Sedona readers. The workflow leverages Databricks notebook widgets to parameterize the cloud provider and dataset location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a8a673-0a00-4fdb-8348-0e147c77b177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../get_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db65718-fe1b-46b9-a548-e893fef8beed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting the current user\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = get_username_from_email(user_email)\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bebbf85-792b-45f4-a34c-5df3d65bbca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cloud_provider = dbutils.widgets.get(\"cloud_provider\")\n",
    "print(f\"Cloud Provider: {cloud_provider}\")\n",
    "\n",
    "if cloud_provider == \"azure\":\n",
    "    dataset_storage_account_name=\"melikadatabricksstorage\"\n",
    "    dataset_container_name=\"geospatial-dataset\"\n",
    "    dataset_dir=\"vector/uk\"\n",
    "elif cloud_provider == \"aws\":\n",
    "    dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "    dataset_input_dir=\"geospatial-dataset/vector/uk\"\n",
    "elif cloud_provider == \"volume\":\n",
    "    schema_name = \"inputs\"\n",
    "    volume_name=\"geospatial_dataset\"\n",
    "    dataset_input_dir = \"vector/uk\"\n",
    "\n",
    "\n",
    "catalog_name = \"geospatial\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf5ef04-9110-473e-973b-75791b564f0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code initializes the Apache Sedona context for geospatial processing in Spark.\n",
    "# It configures the required Sedona and GeoTools packages, then creates a SedonaContext object for reading and manipulating geospatial data in Spark DataFrames.\n",
    "\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.7.1-28.5'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd51711-8b58-44cf-8dda-713d234153a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code iterates through a dictionary of schemas, GeoPackage files, and layer names to ingest geospatial data from cloud storage (Azure, AWS, or Unity Catalog volume) using Apache Sedona. \n",
    "# For each layer in each GeoPackage file, it reads the data into a Spark DataFrame, then writes the DataFrame as a Unity Catalog table with a username-specific suffix, overwriting any existing table. \n",
    "# The storage path is dynamically constructed based on the selected cloud provider.\n",
    "\n",
    "schema_tables = {\n",
    "    \"lookups\": {\n",
    "        \"bdline_gb.gpkg\": [\"boundary_line_ceremonial_counties\"],\n",
    "    },\n",
    "    \"greenspaces\": {\n",
    "        \"opgrsp_gb.gpkg\": [\"greenspace_site\", \"access_point\"]\n",
    "    },\n",
    "    \"networks\": {\n",
    "        \"oproad_gb.gpkg\": [\"road_link\", \"road_node\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for schema, files in schema_tables.items():\n",
    "    for gpkg_file, layers in files.items():\n",
    "        for table_name in layers:\n",
    "            if cloud_provider == \"azure\":\n",
    "                df = sedona.read.format(\"geopackage\").option(\"tableName\", table_name).load(f\"abfss://{dataset_container_name}@{dataset_storage_account_name}.dfs.core.windows.net/{dataset_dir}/{gpkg_file}\")\n",
    "            elif cloud_provider == \"aws\":\n",
    "                df = sedona.read.format(\"geopackage\").option(\"tableName\", table_name).load(f\"s3://{dataset_bucket_name}/{dataset_input_dir}/{gpkg_file}\")\n",
    "            elif cloud_provider == \"volume\":\n",
    "                df = sedona.read.format(\"geopackage\").option(\"tableName\", table_name).load(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/{dataset_input_dir}/{gpkg_file}\")\n",
    "            df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema}.{table_name}_{username}\")\n",
    "            print(f\"Table {catalog_name}.{schema}.{table_name}_{username} is created, yay!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6362237681218965,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-ingest-bronze-sedona",
   "widgets": {
    "cloud_provider": {
     "currentValue": "volume",
     "nuid": "2f019d87-ceae-45b3-a08f-9efaf8c86e35",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "aws",
      "label": "Cloud Provider",
      "name": "cloud_provider",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "aws",
      "label": "Cloud Provider",
      "name": "cloud_provider",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
