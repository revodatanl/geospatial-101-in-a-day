{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da03e87f-3afe-4e6a-9104-22f8acc23173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pdal\n",
    "import json\n",
    "import io\n",
    "import pyarrow as pa\n",
    "from pyspark.sql.functions import col, sqrt, pow, lit, when, atan2, degrees, floor\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, FloatType, IntegerType, ShortType, LongType, ByteType, BooleanType, MapType, StringType, ArrayType\n",
    "import pandas as pd\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import base64\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86aac419-5390-4b26-9e71-2f81cba16a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Username: {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b116f2f8-4da0-4c82-ad42-a772b43e1069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.7.1-28.5'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f766136-a1be-4592-85db-8b1c2d6beaf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "dataset_input_dir=\"geospatial-dataset/point-cloud/washington\"\n",
    "gpkg_file = \"grid/pc_grid.gpkg\"\n",
    "pointcloud_file = \"las-laz/1816.las\"\n",
    "input_path = f\"s3://{dataset_bucket_name}/{dataset_input_dir}/{pointcloud_file}\"\n",
    "\n",
    "df_grid = sedona.read.format(\"geopackage\").option(\"tableName\", \"grid\").load(f\"s3://{dataset_bucket_name}/{dataset_input_dir}/{gpkg_file}\").withColumnRenamed(\"geom\", \"geometry\").withColumn(\"x1\", F.expr(\"ST_X(geometry)\")).withColumn(\"y1\", F.expr(\"ST_Y(geometry)\")).select(\"fid\", \"x1\", \"y1\", \"geometry\").filter(\"fid = 105\")\n",
    "\n",
    "num_partitions = math.ceil(df_grid.count()/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b2ba31-d098-4bbb-9e16-606bba459347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_grid.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080775fa-33a5-4b5f-8b7d-9ff2c38a6649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"access_key\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"secret_key\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'eu-west-2'      # Match your bucket region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d6655d-7d4b-42fa-b570-53a1300eda23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "asprs_classes = {\n",
    "    0: \"Created, Never Classified\",\n",
    "    1: \"Unclassified\",\n",
    "    2: \"Ground\",\n",
    "    3: \"Low Vegetation\",\n",
    "    4: \"Medium Vegetation\",\n",
    "    5: \"High Vegetation\",\n",
    "    6: \"Building\",\n",
    "    7: \"Low Noise\",\n",
    "    8: \"Model Keypoint\",\n",
    "    9: \"Water\",\n",
    "    10: \"Rail\",\n",
    "    11: \"Road Surface\",\n",
    "    12: \"Overlap\",\n",
    "    13: \"Wire Guard\",\n",
    "    14: \"Wire Conductor\",\n",
    "    15: \"Transmission Tower\",\n",
    "    16: \"Wire Structure\",\n",
    "    17: \"Bridge Deck\",\n",
    "    18: \"High Noise\",\n",
    "    20: \"Reserved\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a29b31-7c15-4193-bfb7-ceb5236d0ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_arrow_schema_from_pdal(pdal_array):\n",
    "    \"\"\"Create Arrow schema from PDAL array structure.\"\"\"\n",
    "    fields = []\n",
    "    \n",
    "    # Map PDAL types to Arrow types\n",
    "    type_mapping = {\n",
    "        'float32': pa.float32(),\n",
    "        'float64': pa.float64(),\n",
    "        'int32': pa.int32(),\n",
    "        'int16': pa.int16(),\n",
    "        'uint8': pa.uint8(),\n",
    "        'uint16': pa.uint16(),\n",
    "        'uint32': pa.uint32()\n",
    "    }\n",
    "    \n",
    "    for field_name in pdal_array.dtype.names:\n",
    "        field_type = pdal_array[field_name].dtype\n",
    "        arrow_type = type_mapping.get(str(field_type), pa.float32())  # default to float32\n",
    "        fields.append((field_name, arrow_type))\n",
    "    \n",
    "    return pa.schema(fields)\n",
    "\n",
    "def _create_spark_schema(arrow_schema):\n",
    "    \"\"\"Convert PyArrow schema to Spark DataFrame schema.\"\"\"\n",
    "    spark_fields = []\n",
    "    \n",
    "    type_mapping = {\n",
    "        pa.float32(): FloatType(),\n",
    "        pa.float64(): DoubleType(),\n",
    "        pa.int32(): IntegerType(),\n",
    "        pa.int16(): ShortType(),\n",
    "        pa.int8(): ByteType(),\n",
    "        pa.uint8(): ByteType(),\n",
    "        pa.uint16(): IntegerType(),  # Spark doesn't have unsigned types\n",
    "        pa.uint32(): LongType(),     # Spark doesn't have unsigned types\n",
    "        pa.string(): StringType(),\n",
    "        # Add other type mappings as needed\n",
    "    }\n",
    "    \n",
    "    for field in arrow_schema:\n",
    "        arrow_type = field.type\n",
    "        spark_type = type_mapping.get(arrow_type, StringType())  # default to StringType\n",
    "        spark_fields.append(\n",
    "            StructField(field.name, spark_type, nullable=True)\n",
    "        )\n",
    "    \n",
    "    return StructType(spark_fields)\n",
    "\n",
    "\n",
    "def pdal_to_spark_dataframe_large(pipeline_config, spark, chunk_size=1000000):\n",
    "    \"\"\"Streaming version for very large files.\"\"\"\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_config))\n",
    "    pipeline.execute()\n",
    "    \n",
    "    # Get schema from first array\n",
    "    first_array = pipeline.arrays[0]\n",
    "    schema = _create_arrow_schema_from_pdal(first_array)\n",
    "    \n",
    "    # Create empty RDD\n",
    "    rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    \n",
    "    # Process arrays in chunks\n",
    "    for array in pipeline.arrays:\n",
    "        for i in range(0, len(array), chunk_size):\n",
    "            chunk = array[i:i+chunk_size]\n",
    "            data_dict = {name: chunk[name] for name in chunk.dtype.names}\n",
    "            arrow_table = pa.Table.from_pydict(data_dict, schema=schema)\n",
    "            pdf = arrow_table.to_pandas()\n",
    "            chunk_rdd = spark.sparkContext.parallelize(pdf.to_dict('records'))\n",
    "            rdd = rdd.union(chunk_rdd)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return spark.createDataFrame(rdd, schema=_create_spark_schema(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6f3526-fc45-4783-948f-4a6c4a03a4e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_config = {\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": input_path,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df_pc = pdal_to_spark_dataframe_large(pipeline_config, spark)\n",
    "df_pc.write.mode(\"overwrite\").saveAsTable(f\"geospatial.pointcloud.wasahington_pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd685ab-a320-493e-96f5-b0d5d8c9c85f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pc = spark.table(\"geospatial.pointcloud.wasahington_pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03f248a-74ef-4472-a0cc-f5829d950471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pc = df_pc.withColumn(\"geometry\", F.expr(\"ST_Point(X, Y)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e766b00b-b1c9-45b3-9016-bf45551b8556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dome_radius = 100\n",
    "height_radius = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b919d4fd-d61f-469a-a0fe-2e7bf8d3739d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_selected = df_pc.select(\"X\", \"Y\", \"Z\", \"Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4afaa8-cde8-497d-bd11-f9f30909b9fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_selected.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721540b6-db3c-4cd2-bd1d-932a4f69c651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temp views\n",
    "df_pc.createOrReplaceTempView(\"pc_vw\")\n",
    "df_grid.createOrReplaceTempView(\"grid_vw\")\n",
    "\n",
    "# Perform spatial join using ST_DWithin with 100 meters\n",
    "grid_join_pc = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        g.fid, \n",
    "        ST_X(g.geometry) AS x1,\n",
    "        ST_Y(g.geometry) AS y1,\n",
    "        p.classification,\n",
    "        p.x AS pc_x,\n",
    "        p.y AS pc_y,\n",
    "        p.z AS pc_z,\n",
    "        ST_Distance(g.geometry, p.geometry) AS distance,\n",
    "        g.geometry AS g_geometry,\n",
    "        p.geometry AS pc_geometry \n",
    "    FROM grid_vw g\n",
    "    JOIN pc_vw p\n",
    "        ON ST_DWithin(g.geometry, p.geometry, {dome_radius})\n",
    "    WHERE p.classification IN (5, 6) OR (p.classification = 2 AND ST_DWithin(g.geometry, p.geometry, {height_radius}))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db524ff-0fbe-42f5-b9f1-ad6c51fbb4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter only classification 2 and 6 and count occurrences of (fid, classification)\n",
    "grouped = grid_join_pc.filter(\n",
    "    (F.col(\"classification\").isin(2, 6)) & (F.col(\"distance\") <= height_radius)\n",
    ").groupBy(\"fid\", \"classification\").count()\n",
    "\n",
    "# Define window: partition by fid, order by count descending\n",
    "window_spec = Window.partitionBy(\"fid\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Apply row_number\n",
    "ranked = grouped.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the most frequent classification per fid\n",
    "g_classification_df = ranked.filter(F.col(\"rn\") == 1).select(\"fid\", \"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a63d8f-deac-4669-b62b-6dae389a8bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute the average elevation for each grid point using nearby point cloud data within a specified radius.\n",
    "grid_pc_elevation = grid_join_pc.join(g_classification_df, on=[\"fid\", \"classification\"]).filter(\n",
    "    (F.col(\"distance\") <= height_radius)\n",
    ").groupBy(\"fid\").agg(\n",
    "    (F.sum(\"pc_z\") / F.count(\"pc_z\")).alias(\"height\")\n",
    ")\n",
    "\n",
    "# Combine point cloud data with classification info and computed height, optimized with repartitioning.\n",
    "grid_pc_elevation_all = grid_join_pc.withColumnRenamed(\"classification\", \"p_classification\").join(g_classification_df, on=[\"fid\"]).join(grid_pc_elevation, on=[\"fid\"]).repartitionByRange(num_partitions, \"fid\")\n",
    "\n",
    "# Filter out ground points (e.g., class 2) to retain only buildings and high vegetation points for analysis.\n",
    "grid_pc_cleaned = grid_pc_elevation_all.filter(\"p_classification != 2\").repartitionByRange(num_partitions, \"fid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c9db91-f0cb-4f63-955c-56d33bfcc18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate raw azimuth angle between each grid point and point cloud point, then shift by -90° to align with compass.\n",
    "grid_pc_az = grid_pc_cleaned.withColumn(\n",
    "    \"azimuth_raw\",\n",
    "    degrees(F.atan2(F.col(\"pc_y\") - F.col(\"y1\"), F.col(\"pc_x\") - F.col(\"x1\")))- 90\n",
    ")\n",
    "\n",
    "# Normalize azimuth angle to range [0, 360).\n",
    "grid_pc_az = grid_pc_az.withColumn(\n",
    "    \"azimuth\",\n",
    "    when(F.col(\"azimuth_raw\") < 0, F.col(\"azimuth_raw\") + 360).otherwise(F.col(\"azimuth_raw\"))\n",
    ")\n",
    "\n",
    "# Remove intermediate column to clean up the DataFrame.\n",
    "grid_pc_az = grid_pc_az.drop(\"azimuth_raw\")\n",
    "\n",
    "# Compute elevation angle from each grid point to the point cloud point.\n",
    "grid_pc_az = grid_pc_az.withColumn(\n",
    "    \"elevation\",\n",
    "    degrees(F.atan2(F.col(\"pc_z\") - F.col(\"height\") / 1000, F.col(\"distance\")))\n",
    ")\n",
    "\n",
    "# Bin azimuth values into 2-degree intervals (180 bins total).\n",
    "grid_pc_az = grid_pc_az.withColumn(\"azimuth_bin\", F.floor(F.col(\"azimuth\") / 2))\n",
    "\n",
    "# Get minimum elevation value from the dataset.\n",
    "min_val = F.lit(grid_pc_az.select(F.min(\"elevation\")).first()[0])\n",
    "\n",
    "# Get maximum elevation value from the dataset.\n",
    "max_val = F.lit(grid_pc_az.select(F.max(\"elevation\")).first()[0])\n",
    "\n",
    "# Compute elevation bin width for 90 elevation bins.\n",
    "bin_width = (max_val - min_val) / 89\n",
    "\n",
    "# Assign each point to an elevation bin, clamping values to [0, 89] range.\n",
    "grid_pc_az = grid_pc_az.withColumn(\"elevation_bin\", \n",
    "    F.least(\n",
    "        F.greatest(\n",
    "            F.floor(\n",
    "                (F.col(\"elevation\") - F.lit(min_val)) / \n",
    "                F.lit((max_val - min_val)/90)\n",
    "            ).cast(\"int\"),\n",
    "            F.lit(0)  # Minimum bin is 0\n",
    "        ),\n",
    "        F.lit(89)  # Maximum bin is 89\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f634f2-d097-4f91-9db5-85beb290fe48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate raw azimuth angle (in degrees) from each grid point to each point in the point cloud.\n",
    "# Shifted by -90 to align with the 0° direction being north.\n",
    "grid_pc_az = grid_pc_cleaned.withColumn(\n",
    "    \"azimuth_raw\",\n",
    "    degrees(F.atan2(F.col(\"pc_y\") - F.col(\"y1\"), F.col(\"pc_x\") - F.col(\"x1\"))) - 90\n",
    ")\n",
    "\n",
    "# Normalize azimuth angle to fall within the range [0, 360).\n",
    "grid_pc_az = grid_pc_az.withColumn(\n",
    "    \"azimuth\",\n",
    "    when(F.col(\"azimuth_raw\") < 0, F.col(\"azimuth_raw\") + 360).otherwise(F.col(\"azimuth_raw\"))\n",
    ")\n",
    "\n",
    "# Drop the temporary azimuth_raw column to clean up the DataFrame.\n",
    "grid_pc_az = grid_pc_az.drop(\"azimuth_raw\")\n",
    "\n",
    "# Calculate the elevation angle (in degrees) from the grid point to each point in the point cloud.\n",
    "# Height is divided by 1000 to convert from millimeters to meters if necessary.\n",
    "grid_pc_az = grid_pc_az.withColumn(\n",
    "    \"elevation\",\n",
    "    degrees(F.atan2(F.col(\"pc_z\") - F.col(\"height\") / 1000, F.col(\"distance\")))\n",
    ")\n",
    "\n",
    "# Bin azimuth angles into 2-degree intervals (0–179 bins for 360°).\n",
    "grid_pc_az = grid_pc_az.withColumn(\"azimuth_bin\", F.floor(F.col(\"azimuth\") / 2))\n",
    "\n",
    "# Get the minimum elevation angle across all records to define the lower bound of elevation bins.\n",
    "min_val = F.lit(grid_pc_az.select(F.min(\"elevation\")).first()[0])\n",
    "\n",
    "# Get the maximum elevation angle across all records to define the upper bound of elevation bins.\n",
    "max_val = F.lit(grid_pc_az.select(F.max(\"elevation\")).first()[0])\n",
    "\n",
    "# Compute bin width by dividing elevation range into 89 equal parts (90 bins total).\n",
    "bin_width = (max_val - min_val) / 89\n",
    "\n",
    "# Bin elevation angles into 90 intervals, ensuring they stay within the [0, 89] range.\n",
    "grid_pc_az = grid_pc_az.withColumn(\"elevation_bin\", \n",
    "    F.least(\n",
    "        F.greatest(\n",
    "            F.floor(\n",
    "                (F.col(\"elevation\") - F.lit(min_val)) / \n",
    "                F.lit((max_val - min_val)/90)\n",
    "            ).cast(\"int\"),\n",
    "            F.lit(0)  # Clamp minimum bin index to 0\n",
    "        ),\n",
    "        F.lit(89)  # Clamp maximum bin index to 89\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2215ad0-1fe1-4e35-bf05-d86e34f30007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_az.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a525ab-5e80-4e0f-852f-9dd16937724b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window that partitions the data by azimuth and elevation bins,\n",
    "# and orders points within each bin by their distance to the grid point.\n",
    "window_spec = Window.partitionBy(\"azimuth_bin\", \"elevation_bin\").orderBy(\"distance\")\n",
    "\n",
    "# Assign a row number within each azimuth-elevation bin, so the closest point (smallest distance) gets rank 1.\n",
    "df_with_rank = grid_pc_az.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "\n",
    "# Keep only the closest point (rank 1) in each bin and drop the temporary rank column.\n",
    "# Then repartition the result by 'fid' to optimize parallel processing in subsequent steps.\n",
    "closest_points = df_with_rank.filter(col(\"rn\") == 1).drop(\"rn\").repartitionByRange(num_partitions, \"fid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa78bbd5-9762-4b18-8aaf-e73b985ebd4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dome(pdf: pd.DataFrame, max_azimuth: int = 180, max_elevation: int = 90) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a dome matrix based on azimuth and elevation bins, with obstruction handling for buildings.\n",
    "    \"\"\"\n",
    "    dome = np.zeros((max_azimuth, max_elevation), dtype=int)\n",
    "    domeDists = np.zeros((max_azimuth, max_elevation), dtype=float)\n",
    "\n",
    "    for _, row in pdf.iterrows():\n",
    "        a = int(row[\"azimuth_bin\"])\n",
    "        e = int(row[\"elevation_bin\"])\n",
    "        dome[a, e] = row[\"p_classification\"]\n",
    "        domeDists[a, e] = row[\"distance\"]\n",
    "\n",
    "    # Mark parts of the dome that are obstructed by buildings\n",
    "    if np.any(dome == 6):  # 6 = buildings\n",
    "        bhor, bver = np.where(dome == 6)\n",
    "        builds = np.stack((bhor, bver), axis=-1)\n",
    "        shape = (builds.shape[0] + 1, builds.shape[1])\n",
    "        builds = np.append(builds, (bhor[0], bver[0])).reshape(shape)\n",
    "        azimuth_change = builds[:, 0][:-1] != builds[:, 0][1:]\n",
    "        keep = np.where(azimuth_change)\n",
    "        roof_rows, roof_cols = builds[keep][:, 0], builds[keep][:, 1]\n",
    "        for roof_row, roof_col in zip(roof_rows, roof_cols):\n",
    "            condition = np.where(np.logical_or(\n",
    "                domeDists[roof_row, :roof_col] > domeDists[roof_row, roof_col],\n",
    "                dome[roof_row, :roof_col] == 0\n",
    "            ))\n",
    "            dome[roof_row, :roof_col][condition] = 6\n",
    "\n",
    "    return dome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c044bd5-68a6-486d-800a-cd862095863e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot dome\n",
    "def generate_plot_image(dome):\n",
    "    # Create circular grid\n",
    "    theta = np.linspace(0, 2*np.pi, 180, endpoint=False)\n",
    "    radius = np.linspace(0, 90, 90)\n",
    "    theta_grid, radius_grid = np.meshgrid(theta, radius)\n",
    "\n",
    "    Z = dome.copy().astype(float)\n",
    "    \n",
    "    Z = Z.T[::-1, :]  # Transpose and flip vertically\n",
    "\n",
    "    Z[Z == 0] = 0\n",
    "    Z[np.isin(Z, [5])] = 0.5\n",
    "    Z[Z == 6] = 1\n",
    "\n",
    "    if Z[Z == 6].size == 0:\n",
    "        Z[0, 0] = 1  # Force plot to show something\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111, projection='polar')\n",
    "    cmap = plt.get_cmap('tab20c')\n",
    "    ax.pcolormesh(theta, radius, Z, cmap=cmap)\n",
    "    ax.set_ylim([0, 90])\n",
    "    ax.tick_params(labelleft=False)\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    return img_base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed33bf68-1e67-4538-b356-cdce9eecec79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_SVF(radius, dome):\n",
    "    obstructedArea = 0\n",
    "    treeObstruction = 0\n",
    "    buildObstruction = 0\n",
    "    for i in range(0, 180):\n",
    "        for j in range(0, 90):\n",
    "            if dome[i, j] != 0:\n",
    "                v = 90 - (j + 1)\n",
    "                R = math.cos(v * math.pi / 180) * radius\n",
    "                r = math.cos((v + 1) * math.pi / 180) * radius\n",
    "                # calculate area of each obstructed sector (circular sector area calculation)\n",
    "                cell_area = (math.pi / 180.0) * (R ** 2 - r ** 2)\n",
    "                obstructedArea += cell_area\n",
    "                if dome[i, j] in [5]:\n",
    "                    treeObstruction += cell_area\n",
    "                elif dome[i, j] == 6:\n",
    "                    buildObstruction += cell_area\n",
    "    circleArea = math.pi * (radius ** 2)\n",
    "    # SVF: proportion of open area to total area\n",
    "    SVF = (circleArea - obstructedArea) / circleArea\n",
    "    treeObstructionPercentage = treeObstruction / circleArea\n",
    "    buildObstructionPercentage = buildObstruction / circleArea\n",
    "    return SVF, treeObstructionPercentage, buildObstructionPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067bf2a7-1adb-40ae-8a73-106b5624977b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_and_plot(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    fid = pdf[\"fid\"].iloc[0]\n",
    "\n",
    "    # Create dome with building/vegetation obstruction\n",
    "    dome = create_dome(pdf)\n",
    "\n",
    "    # Generate base64-encoded fisheye plot image\n",
    "    plot_base64 = generate_plot_image(dome)\n",
    "\n",
    "    # Compute SVF and obstruction metrics\n",
    "    SVF, tree_percentage, build_percentage = calculate_SVF(100, dome)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        [[fid, dome.tolist(), plot_base64, SVF, tree_percentage, build_percentage]],\n",
    "        columns=[\"fid\", \"dome\", \"plot\", \"SVF\", \"treeObstruction\", \"buildObstruction\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7ea0f6-8608-4cd7-91be-a41ff973f983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Desired schema\n",
    "output_schema = StructType([\n",
    "    StructField(\"fid\", IntegerType()),\n",
    "    StructField(\"dome\", ArrayType(ArrayType(IntegerType()))),\n",
    "    StructField(\"plot\", StringType()),\n",
    "    StructField(\"SVF\", FloatType()),\n",
    "    StructField(\"treeObstruction\", FloatType()),\n",
    "    StructField(\"buildObstruction\", FloatType())\n",
    "])\n",
    "\n",
    "result_df = closest_points.groupBy(\"fid\").applyInPandas(process_and_plot, schema=output_schema)\n",
    "result_df.write.mode(\"overwrite\").saveAsTable(f\"geospatial.pointcloud.wasahington_grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2a8b0e-f36e-4a8f-b812-586680811679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = spark.table(\"geospatial.pointcloud.wasahington_grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39187717-56d2-410c-9e35-df627115d00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b380715-4e90-4996-8aac-9f5de352448f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = result_df.filter((F.col(\"fid\") == 105) )\n",
    "pdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd3af306-ab9d-4a52-a5c0-e2369efed62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch the the grid point with fid = 105 for a sample visualization\n",
    "pdf = result_df.filter(\"fid = 105\").select(\"fid\", \"plot\").toPandas()\n",
    "\n",
    "for index, row in pdf.iterrows():\n",
    "  # Decode base64 string to bytes\n",
    "  img_bytes = base64.b64decode(img_base64)\n",
    "\n",
    "  # Load image with PIL\n",
    "  image = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "  # Display using matplotlib (preserves original colors)\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')  # Hide axes\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6631496815146743,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "09-pointclouds-pdal-pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
