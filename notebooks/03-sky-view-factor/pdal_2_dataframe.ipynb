{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdea07d-a760-4252-b5ef-488f24675ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48af9da-f1c7-44cb-a10c-1bb9a1f91faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"access_key\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"secret_key\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'eu-west-2'      # Match your bucket region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37898f1a-086e-4979-af73-9ca1aab51a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_arrow_schema_from_pdal(pdal_array):\n",
    "    \"\"\"Create Arrow schema from PDAL array structure.\"\"\"\n",
    "    fields = []\n",
    "    \n",
    "    # Map PDAL types to Arrow types\n",
    "    type_mapping = {\n",
    "        'float32': pa.float32(),\n",
    "        'float64': pa.float64(),\n",
    "        'int32': pa.int32(),\n",
    "        'int16': pa.int16(),\n",
    "        'uint8': pa.uint8(),\n",
    "        'uint16': pa.uint16(),\n",
    "        'uint32': pa.uint32()\n",
    "    }\n",
    "    \n",
    "    for field_name in pdal_array.dtype.names:\n",
    "        field_type = pdal_array[field_name].dtype\n",
    "        arrow_type = type_mapping.get(str(field_type), pa.float32())  # default to float32\n",
    "        fields.append((field_name, arrow_type))\n",
    "    \n",
    "    return pa.schema(fields)\n",
    "\n",
    "def _create_spark_schema(arrow_schema):\n",
    "    \"\"\"Convert PyArrow schema to Spark DataFrame schema.\"\"\"\n",
    "    spark_fields = []\n",
    "    \n",
    "    type_mapping = {\n",
    "        pa.float32(): FloatType(),\n",
    "        pa.float64(): DoubleType(),\n",
    "        pa.int32(): IntegerType(),\n",
    "        pa.int16(): ShortType(),\n",
    "        pa.int8(): ByteType(),\n",
    "        pa.uint8(): ByteType(),\n",
    "        pa.uint16(): IntegerType(),  # Spark doesn't have unsigned types\n",
    "        pa.uint32(): LongType(),     # Spark doesn't have unsigned types\n",
    "        pa.string(): StringType(),\n",
    "        # Add other type mappings as needed\n",
    "    }\n",
    "    \n",
    "    for field in arrow_schema:\n",
    "        arrow_type = field.type\n",
    "        spark_type = type_mapping.get(arrow_type, StringType())  # default to StringType\n",
    "        spark_fields.append(\n",
    "            StructField(field.name, spark_type, nullable=True)\n",
    "        )\n",
    "    \n",
    "    return StructType(spark_fields)\n",
    "\n",
    "\n",
    "def pdal_to_spark_dataframe_large(pipeline_config, spark, chunk_size=1000000):\n",
    "    \"\"\"Streaming version for very large files.\"\"\"\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_config))\n",
    "    pipeline.execute()\n",
    "    \n",
    "    # Get schema from first array\n",
    "    first_array = pipeline.arrays[0]\n",
    "    schema = _create_arrow_schema_from_pdal(first_array)\n",
    "    \n",
    "    # Create empty RDD\n",
    "    rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    \n",
    "    # Process arrays in chunks\n",
    "    for array in pipeline.arrays:\n",
    "        for i in range(0, len(array), chunk_size):\n",
    "            chunk = array[i:i+chunk_size]\n",
    "            data_dict = {name: chunk[name] for name in chunk.dtype.names}\n",
    "            arrow_table = pa.Table.from_pydict(data_dict, schema=schema)\n",
    "            pdf = arrow_table.to_pandas()\n",
    "            chunk_rdd = spark.sparkContext.parallelize(pdf.to_dict('records'))\n",
    "            rdd = rdd.union(chunk_rdd)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return spark.createDataFrame(rdd, schema=_create_spark_schema(schema))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pdal_2_dataframe",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
