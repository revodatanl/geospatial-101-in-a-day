{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72793edb-352d-4817-bb68-562d8816ca33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../get_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f20d02-71b9-42c7-9251-726c92087c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "username = get_username_from_email(user_email)\n",
    "print(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06483e13-4ccb-4df7-9d36-9f277b938d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import expr, udf, col\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb1d83f8-5b63-4532-90e8-45ec30e7811f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.7.1-28.5'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce93ada-ecfd-4e24-8002-e2dfc0a54174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "dataset_dir= \"geospatial-dataset/netcdf/global\"\n",
    "dataset_file = \"tos_O1_2001-2002.nc\"\n",
    "nc_variable = \"tos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df02885-c822-4267-b2e1-c20bc0c82367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load NetCDF file from S3, extract raster bands, and display raster information\n",
    "\n",
    "df = sedona.read.format(\"binaryFile\").load(\n",
    "    f\"s3://{dataset_bucket_name}/{dataset_dir}/{dataset_file}\"\n",
    ")\n",
    "record_info_row = df.selectExpr(\"RS_NetCDFInfo(content) as record_info\").first()\n",
    "print(record_info_row)\n",
    "\n",
    "df_raster = df.withColumn(\n",
    "    \"raster\", expr(f\"RS_FromNetCDF(content, '{nc_variable}')\")\n",
    ")\n",
    "display(df_raster)\n",
    "\n",
    "df_num = df_raster.withColumn(\"num_raster\", expr(\"RS_NumBands(raster)\"))\n",
    "num_bands = df_num.select(\"num_raster\").first()[0]\n",
    "print(\"Number of bands\", num_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf77dc5-8414-4454-8a3c-c949b5b1fb73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explore\n",
    "htmlDF = df_raster.selectExpr(\"RS_AsImage(raster, 500) as raster_image\")\n",
    "SedonaUtils.display_image(htmlDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb14df4-68c9-466c-a1e7-536e6c5a6c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract pixel centroids from the first raster band and select geometry and coordinates\n",
    "\n",
    "df_pixels_as_geom = (\n",
    "    df_raster\n",
    "    .selectExpr(\"explode(RS_PixelAsCentroids(raster, 1)) as exploded\")\n",
    "    .selectExpr(\n",
    "        \"exploded.geom as geom\",\n",
    "        \"exploded.x as x\",\n",
    "        \"exploded.y as y\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_pixels_as_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59fb681-27ee-4660-953d-56181eec0d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code processes raster bands from a NetCDF file using Sedona.\n",
    "# It extracts pixel polygons for each band, filters out invalid values,\n",
    "# and joins all bands into a single DataFrame for further analysis.\n",
    "\n",
    "def process_band(df_raster, band):\n",
    "    return (\n",
    "        df_raster\n",
    "        .selectExpr(f\"explode_outer(RS_PixelAsPolygons(raster, {band})) as band\")\n",
    "        .selectExpr(\"band.geom as geom\", \"band.value as band_value\", \"band.x as x\", \"band.y as y\")\n",
    "        .where(\"band_value != 100000002004087734272\")\n",
    "        .repartition(100)\n",
    "    )\n",
    "\n",
    "bands_df = process_band(df_raster, 1).withColumnRenamed(\"band_value\", \"band_1\")\n",
    "\n",
    "for band in range(2, num_bands + 1):\n",
    "    band_df = process_band(df_raster, band).drop(\"geom\")\n",
    "    bands_df = (\n",
    "        bands_df\n",
    "        .join(band_df, on=[\"x\", \"y\"], how=\"inner\")\n",
    "        .withColumnRenamed(\"band_value\", f\"band_{band}\")\n",
    "    )\n",
    "\n",
    "display(bands_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec027c9-f0d0-42ce-97ca-956331be9f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate per-pixel statistics (mean, stddev, min, max) across all raster bands\n",
    "\n",
    "band_cols = [f\"band_{i}\" for i in range(1, num_bands + 1)]\n",
    "\n",
    "df_stats = (\n",
    "    bands_df\n",
    "    .withColumn(\n",
    "        \"band_mean\",\n",
    "        F.expr(f\"aggregate(array({', '.join(band_cols)}), 0D, (acc, x) -> acc + x) / {len(band_cols)}\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"band_stddev\",\n",
    "        F.expr(\n",
    "            f\"\"\"sqrt(\n",
    "                aggregate(array({', '.join(band_cols)}), 0D, (acc, x) -> acc + pow(x - band_mean, 2)) / {len(band_cols)}\n",
    "            )\"\"\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"band_min\", F.least(*band_cols))\n",
    "    .withColumn(\"band_max\", F.greatest(*band_cols))\n",
    ")\n",
    "\n",
    "df_stats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a93936-7f84-43a9-bca0-c65a7cd94419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to compute the specified percentile of a list of values\n",
    "def compute_percentile(values, percentile):\n",
    "    return float(np.percentile(values, percentile))\n",
    "\n",
    "# Create a UDF to compute the 25th percentile\n",
    "percentile_udf = udf(lambda row: compute_percentile(row, 25), DoubleType())\n",
    "df_stats = df_stats.withColumn(\"band_p25\", percentile_udf(F.array(*band_cols)))\n",
    "\n",
    "# Create a UDF to compute the 50th percentile\n",
    "percentile_udf = udf(lambda row: compute_percentile(row, 50), DoubleType())\n",
    "df_stats = df_stats.withColumn(\"band_p50\", percentile_udf(F.array(*band_cols)))\n",
    "\n",
    "# Create a UDF to compute the 75th percentile\n",
    "percentile_udf = udf(lambda row: compute_percentile(row, 75), DoubleType())\n",
    "df_stats = df_stats.withColumn(\"band_p75\", percentile_udf(F.array(*band_cols)))\n",
    "\n",
    "# Repartition the DataFrame to a single partition\n",
    "df_stats = df_stats.repartition(1)\n",
    "df_stats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abbb4afd-6cd7-4305-a8de-244927f77fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the df_stats DataFrame as a GeoJSON file to S3, overwriting any existing file\n",
    "df_stats.write.mode(\"overwrite\").format(\"geojson\").save(\n",
    "    f\"s3://{dataset_bucket_name}/{dataset_dir}/{nc_variable}_{username}.geojson\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-NetCDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
