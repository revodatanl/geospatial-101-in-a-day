{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da03e87f-3afe-4e6a-9104-22f8acc23173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pdal\n",
    "import json\n",
    "import io\n",
    "import pyarrow as pa\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, BooleanType, MapType\n",
    "from pyspark.sql.functions import col, sqrt, pow, lit, when, atan2, degrees, floor\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, FloatType, IntegerType, ShortType, LongType, ByteType\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pandas as pd\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86aac419-5390-4b26-9e71-2f81cba16a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"Username: {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b116f2f8-4da0-4c82-ad42-a772b43e1069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.3_2.12:1.7.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.7.1-28.5'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d8cf5b-4335-475a-88b4-72aae5c875a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS geospatial.pointcloud;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f766136-a1be-4592-85db-8b1c2d6beaf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_bucket_name = \"revodata-databricks-geospatial\"\n",
    "dataset_input_dir=\"geospatial-dataset/point-cloud/washington/grid\"\n",
    "gpkg_file = \"pc_grid.gpkg\"\n",
    "df_pc_grid = sedona.read.format(\"geopackage\").option(\"tableName\", \"grid\").load(f\"s3://{dataset_bucket_name}/{dataset_input_dir}/{gpkg_file}\").withColumnRenamed(\"geom\", \"geometry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aef8602-4290-4c7e-8136-3eed783f4cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pc_grid = df_pc_grid.limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ece34b-c540-4eb0-b483-0e7cb005d7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pc_grid)\n",
    "\n",
    "# df_grid = df_pc_grid.withColumn(\"x1\", F.expr(\"ST_X(geometry)\")).withColumn(\"y1\", F.expr(\"ST_Y(geometry)\")).select(\"fid\", \"x1\", \"y1\")\n",
    "x = 395176.7\n",
    "y = 136693.5\n",
    "df_pc_grid =spark.sql(f\"\"\"SELECT 1 AS fid, ST_Point({x},{y}) AS geometry\"\"\")\n",
    "df_grid = df_pc_grid.withColumn(\"x1\", F.expr(\"ST_X(geometry)\")).withColumn(\"y1\", F.expr(\"ST_Y(geometry)\")).select(\"fid\", \"x1\", \"y1\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(df_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080775fa-33a5-4b5f-8b7d-9ff2c38a6649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"access_key\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(scope=\"aws_geospatial_s3\", key=\"secret_key\")\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'eu-west-2'      # Match your bucket region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d6655d-7d4b-42fa-b570-53a1300eda23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "asprs_classes = {\n",
    "    0: \"Created, Never Classified\",\n",
    "    1: \"Unclassified\",\n",
    "    2: \"Ground\",\n",
    "    3: \"Low Vegetation\",\n",
    "    4: \"Medium Vegetation\",\n",
    "    5: \"High Vegetation\",\n",
    "    6: \"Building\",\n",
    "    7: \"Low Noise\",\n",
    "    8: \"Model Keypoint\",\n",
    "    9: \"Water\",\n",
    "    10: \"Rail\",\n",
    "    11: \"Road Surface\",\n",
    "    12: \"Overlap\",\n",
    "    13: \"Wire Guard\",\n",
    "    14: \"Wire Conductor\",\n",
    "    15: \"Transmission Tower\",\n",
    "    16: \"Wire Structure\",\n",
    "    17: \"Bridge Deck\",\n",
    "    18: \"High Noise\",\n",
    "    20: \"Reserved\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b894b84-6487-4357-945e-eb45606a0441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(pdal.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728f3214-736c-47fa-a1f1-97796cca2ff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pdal\n",
    "import json\n",
    "\n",
    "# PDAL pipeline to read LAS/LAZ from S3\n",
    "input_path = \"s3://revodata-databricks-geospatial/geospatial-dataset/point-cloud/washington/las-laz/1816.las\"  # or .laz\n",
    "\n",
    "# 2. PDAL pipeline configuration\n",
    "pipeline_config = {\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": input_path,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = pdal.Pipeline(json.dumps(pipeline_config))\n",
    "count = pipeline.execute()\n",
    "print(f\"Successfully processed {count} points to Parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a29b31-7c15-4193-bfb7-ceb5236d0ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_arrow_schema_from_pdal(pdal_array):\n",
    "    \"\"\"Create Arrow schema from PDAL array structure.\"\"\"\n",
    "    fields = []\n",
    "    \n",
    "    # Map PDAL types to Arrow types\n",
    "    type_mapping = {\n",
    "        'float32': pa.float32(),\n",
    "        'float64': pa.float64(),\n",
    "        'int32': pa.int32(),\n",
    "        'int16': pa.int16(),\n",
    "        'uint8': pa.uint8(),\n",
    "        'uint16': pa.uint16(),\n",
    "        'uint32': pa.uint32()\n",
    "    }\n",
    "    \n",
    "    for field_name in pdal_array.dtype.names:\n",
    "        field_type = pdal_array[field_name].dtype\n",
    "        arrow_type = type_mapping.get(str(field_type), pa.float32())  # default to float32\n",
    "        fields.append((field_name, arrow_type))\n",
    "    \n",
    "    return pa.schema(fields)\n",
    "\n",
    "def _create_spark_schema(arrow_schema):\n",
    "    \"\"\"Convert PyArrow schema to Spark DataFrame schema.\"\"\"\n",
    "    spark_fields = []\n",
    "    \n",
    "    type_mapping = {\n",
    "        pa.float32(): FloatType(),\n",
    "        pa.float64(): DoubleType(),\n",
    "        pa.int32(): IntegerType(),\n",
    "        pa.int16(): ShortType(),\n",
    "        pa.int8(): ByteType(),\n",
    "        pa.uint8(): ByteType(),\n",
    "        pa.uint16(): IntegerType(),  # Spark doesn't have unsigned types\n",
    "        pa.uint32(): LongType(),     # Spark doesn't have unsigned types\n",
    "        pa.string(): StringType(),\n",
    "        # Add other type mappings as needed\n",
    "    }\n",
    "    \n",
    "    for field in arrow_schema:\n",
    "        arrow_type = field.type\n",
    "        spark_type = type_mapping.get(arrow_type, StringType())  # default to StringType\n",
    "        spark_fields.append(\n",
    "            StructField(field.name, spark_type, nullable=True)\n",
    "        )\n",
    "    \n",
    "    return StructType(spark_fields)\n",
    "\n",
    "\n",
    "def pdal_to_spark_dataframe_large(pipeline_config, spark, chunk_size=1000000):\n",
    "    \"\"\"Streaming version for very large files.\"\"\"\n",
    "    pipeline = pdal.Pipeline(json.dumps(pipeline_config))\n",
    "    pipeline.execute()\n",
    "    \n",
    "    # Get schema from first array\n",
    "    first_array = pipeline.arrays[0]\n",
    "    schema = _create_arrow_schema_from_pdal(first_array)\n",
    "    \n",
    "    # Create empty RDD\n",
    "    rdd = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    \n",
    "    # Process arrays in chunks\n",
    "    for array in pipeline.arrays:\n",
    "        for i in range(0, len(array), chunk_size):\n",
    "            chunk = array[i:i+chunk_size]\n",
    "            data_dict = {name: chunk[name] for name in chunk.dtype.names}\n",
    "            arrow_table = pa.Table.from_pydict(data_dict, schema=schema)\n",
    "            pdf = arrow_table.to_pandas()\n",
    "            chunk_rdd = spark.sparkContext.parallelize(pdf.to_dict('records'))\n",
    "            rdd = rdd.union(chunk_rdd)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    return spark.createDataFrame(rdd, schema=_create_spark_schema(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab6f3526-fc45-4783-948f-4a6c4a03a4e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDAL to Spark\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PDAL pipeline configuration\n",
    "pipeline_config = {\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "            \"type\": \"readers.las\",\n",
    "            \"filename\": input_path,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df_pc = pdal_to_spark_dataframe_large(pipeline_config, spark)\n",
    "\n",
    "# Show the result\n",
    "display(df_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8334e3c-c6a9-4dcb-8b29-f0b97a8c6102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pc.write.mode(\"overwrite\").saveAsTable(f\"geospatial.pointcloud.wasahington_pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03f248a-74ef-4472-a0cc-f5829d950471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pc = df_pc.withColumn(\"geometry\", F.expr(\"ST_Point(X, Y)\"))\n",
    "display(df_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e766b00b-b1c9-45b3-9016-bf45551b8556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dome_radius = 100\n",
    "height_radius = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b919d4fd-d61f-469a-a0fe-2e7bf8d3739d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_selected = df_pc.select(\"X\", \"Y\", \"Z\", \"Classification\")\n",
    "display(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "721540b6-db3c-4cd2-bd1d-932a4f69c651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register as temp views\n",
    "df_pc.createOrReplaceTempView(\"pc_vw\")\n",
    "df_pc_grid.createOrReplaceTempView(\"pc_grid_vw\")\n",
    "\n",
    "# Perform spatial join using ST_DWithin with 100 meters\n",
    "grid_join_pc = spark.sql(f\"\"\"\n",
    "SELECT g.fid, \n",
    "ST_X(g.geometry) AS x1,\n",
    "ST_Y(g.geometry) AS y1,\n",
    "p.classification,\n",
    "p.x AS pc_x,\n",
    "p.y AS pc_y,\n",
    "p.z AS pc_z,\n",
    "ST_Distance(g.geometry, p.geometry) AS distance,\n",
    "g.geometry AS g_geometry,\n",
    "p.geometry AS pc_geometry \n",
    "FROM pc_grid_vw g\n",
    "JOIN pc_vw p\n",
    "ON ST_DWithin(g.geometry, p.geometry, {dome_radius})\n",
    "WHERE p.classification IN (5, 6) OR (p.classification = 2 AND ST_DWithin(g.geometry, p.geometry, {height_radius}))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c023e9-9dde-48f6-b717-da2132f39bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(grid_join_pc)\n",
    "grid_join_pc.cache()\n",
    "grid_join_pc.createOrReplaceTempView(\"grid_join_pc_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db524ff-0fbe-42f5-b9f1-ad6c51fbb4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter only classification 2 and 6 and count occurrences of (fid, classification)\n",
    "grouped = grid_join_pc.filter(\n",
    "    (F.col(\"classification\").isin(2, 6)) & (F.col(\"distance\") <= height_radius)\n",
    ").groupBy(\"fid\", \"classification\").count()\n",
    "\n",
    "# Define window: partition by fid, order by count descending\n",
    "window_spec = Window.partitionBy(\"fid\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Apply row_number\n",
    "ranked = grouped.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the most frequent classification per fid\n",
    "g_classification_df = ranked.filter(F.col(\"rn\") == 1).select(\"fid\", \"classification\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea657792-889c-431f-85da-81cae999e509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(g_classification_df)\n",
    "g_classification_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a63d8f-deac-4669-b62b-6dae389a8bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_elevation = grid_join_pc.join(g_classification_df, on=[\"fid\", \"classification\"]).filter(\n",
    "    (F.col(\"distance\") <= height_radius)\n",
    ").groupBy(\"fid\").agg(\n",
    "    (F.sum(\"pc_z\") / F.count(\"pc_z\")).alias(\"height\")\n",
    ")\n",
    "\n",
    "\n",
    "display(grid_pc_elevation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92808777-31e2-4353-a6cd-6ddccc8669a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(grid_join_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90daecd7-ec94-4ac5-80d8-bd47fefe65bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_elevation_all = grid_join_pc.withColumnRenamed(\"classification\", \"p_classification\").join(g_classification_df, on=[\"fid\"]).join(grid_pc_elevation, on=[\"fid\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55be8cc4-8bae-4f9b-8b79-fa47692fa60d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(grid_pc_elevation_all)\n",
    "grid_pc_elevation_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd78dd6e-05d2-4eb3-9aeb-3803b3355af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_cleaned = grid_pc_elevation_all.filter(\"p_classification != 2\")\n",
    "grid_pc_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a6062d-1d5e-4834-94ca-20ddfa2b4be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, degrees, atan2, when, floor, round\n",
    "\n",
    "elevation_range = 90  # Total number of bins you want\n",
    "max_elevation = 66    # Your current maximum bin value\n",
    "\n",
    "grid_pc_az = grid_pc_cleaned.withColumn(\n",
    "    \"azimuth_raw\",\n",
    "    degrees(atan2(col(\"pc_y\") - col(\"y1\"), col(\"pc_x\") - col(\"x1\")))- 90\n",
    ").withColumn(\n",
    "    \"azimuth\",\n",
    "    when(col(\"azimuth_raw\") < 0, col(\"azimuth_raw\") + 360).otherwise(col(\"azimuth_raw\"))\n",
    ").drop(\"azimuth_raw\").withColumn(\n",
    "    \"elevation\",\n",
    "    degrees(atan2(col(\"pc_z\") - col(\"height\")/1000, col(\"distance\")))).withColumn(\"azimuth_bin\", floor(col(\"azimuth\") / 2)) \\\n",
    "    .withColumn(\"elevation_bin\", round(col(\"elevation\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f634f2-d097-4f91-9db5-85beb290fe48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, degrees, atan2, when, floor, round\n",
    "\n",
    "\n",
    "grid_pc_az = grid_pc_cleaned.withColumn(\n",
    "    \"azimuth_raw\",\n",
    "    degrees(atan2(col(\"pc_y\") - col(\"y1\"), col(\"pc_x\") - col(\"x1\")))- 90\n",
    ").withColumn(\n",
    "    \"azimuth\",\n",
    "    when(col(\"azimuth_raw\") < 0, col(\"azimuth_raw\") + 360).otherwise(col(\"azimuth_raw\"))\n",
    ").drop(\"azimuth_raw\").withColumn(\n",
    "    \"elevation\",\n",
    "    degrees(atan2(col(\"pc_z\") - col(\"height\")/1000, col(\"distance\")))).withColumn(\"azimuth_bin\", floor(col(\"azimuth\") / 2))\n",
    "    \n",
    "min_val = F.lit(grid_pc_az.select(F.min(\"elevation\")).first()[0])\n",
    "max_val = F.lit(grid_pc_az.select(F.max(\"elevation\")).first()[0])\n",
    "bin_width = (max_val - min_val) / 89  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917d9fa2-3cf5-47cc-b78a-ec66a394a23d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(grid_pc_az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5c9781-5c85-4b5f-affa-e64b99fe0dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_az =  grid_pc_az.withColumn(\"elevation_bin\", F.least(F.greatest(F.floor((F.col(\"elevation\") - min_val) / bin_width).cast(\"int\") + F.lit(1),F.lit(1)),F.lit(90)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a38a514-7fa4-4449-aa96-395288402a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_az = grid_pc_az.withColumn(\"elevation_bin\", \n",
    "    F.least(\n",
    "        F.greatest(\n",
    "            F.floor(\n",
    "                (F.col(\"elevation\") - F.lit(min_val)) / \n",
    "                F.lit((max_val - min_val)/90)\n",
    "            ).cast(\"int\"),\n",
    "            F.lit(0)  # Minimum bin is 0\n",
    "        ),\n",
    "        F.lit(89)  # Maximum bin is 89\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baed00cf-d43b-4aa0-9286-c319f74e9a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_pc_az.createOrReplaceTempView(\"grid_pc_az_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c97aa1-58b8-41ba-b66c-8ffcc73cdc85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  DISTINCT elevation_bin FROM grid_pc_az_vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "febef61d-790a-47f9-a0c8-be6ca8e5f2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"azimuth_bin\", \"elevation_bin\").orderBy(\"distance\")\n",
    "\n",
    "df_with_rank = grid_pc_az.withColumn(\"rn\", F.row_number().over(window_spec))\n",
    "\n",
    "closest_points = df_with_rank.filter(col(\"rn\") == 1).drop(\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad00f968-c425-42c6-bff4-ab15f51434e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(closest_points)\n",
    "closest_points.createOrReplaceTempView(\"closest_points_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16e4120-016c-49d5-a9dd-c8c10b64d8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  DISTINCT elevation_bin FROM closest_points_vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe54670-87d5-4af8-93f5-4a4d624cf585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect as Pandas DataFrame\n",
    "pdf = closest_points.select(\"azimuth_bin\", \"elevation_bin\", \"distance\", \"p_classification\").toPandas()\n",
    "\n",
    "# Find dome grid size (assuming max azimuth_bin=180, elevation_bin=90 as in your original)\n",
    "max_azimuth = int(pdf[\"azimuth_bin\"].max()) + 1\n",
    "max_elevation = int(pdf[\"elevation_bin\"].max()) + 1\n",
    "\n",
    "max_azimuth = 180\n",
    "max_elevation = 90\n",
    "\n",
    "dome = np.zeros((max_azimuth, max_elevation), dtype=int)\n",
    "domeDists = np.zeros((max_azimuth, max_elevation), dtype=float)\n",
    "\n",
    "# Fill dome and domeDists arrays\n",
    "for _, row in pdf.iterrows():\n",
    "    a = int(row[\"azimuth_bin\"])\n",
    "    e = int(row[\"elevation_bin\"])\n",
    "    dome[a, e] = row[\"p_classification\"]\n",
    "    domeDists[a, e] = row[\"distance\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e79f2af-a5b3-4afe-b630-c28db7004776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f885e274-6c33-46c5-b78a-74e5315c72c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if np.any(dome == 6):\n",
    "    bhor, bver = np.where(dome == 6)\n",
    "    builds = np.stack((bhor, bver), axis=-1)\n",
    "    shape = (builds.shape[0] + 1, builds.shape[1])\n",
    "    builds = np.append(builds, (bhor[0], bver[0])).reshape(shape)\n",
    "    azimuth_change = builds[:, 0][:-1] != builds[:, 0][1:]\n",
    "    keep = np.where(azimuth_change)\n",
    "    roof_rows, roof_cols = builds[keep][:, 0], builds[keep][:, 1]\n",
    "    for roof_row, roof_col in zip(roof_rows, roof_cols):\n",
    "        condition = np.where(np.logical_or(domeDists[roof_row, :roof_col] > domeDists[roof_row, roof_col], dome[roof_row, :roof_col] == 0))\n",
    "        dome[roof_row, :roof_col][condition] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c044bd5-68a6-486d-800a-cd862095863e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot dome\n",
    "def plot(dome):\n",
    "    # Create circular grid\n",
    "    theta = np.linspace(0, 2*np.pi, 180, endpoint=False)\n",
    "    radius = np.linspace(0, 90, 90)\n",
    "    theta_grid, radius_grid = np.meshgrid(theta, radius)\n",
    "    Z = dome.copy().astype(float)\n",
    "    print(f\"Z: {Z}\")\n",
    "    Z = Z.T[::-1, 0:]  # Transpose to (90, 180)\n",
    "    # assign colors depending on class\n",
    "    Z[Z == 0] = 0\n",
    "    Z[np.isin(Z, [5])] = 0.5  # Classes 2-5\n",
    "    Z[Z == 6] = 1\n",
    "    if Z[Z == 6].size == 0:\n",
    "        Z[0,0] = 1\n",
    "        # Verify dimensions\n",
    "    print(f\"theta_grid: {theta.shape}, radius_grid: {radius.shape}, Z: {Z.shape}\")\n",
    "    axes = plt.subplot(111, projection='polar')\n",
    "    cmap = plt.get_cmap('tab20c')\n",
    "    axes.pcolormesh(theta, radius, Z, cmap=cmap)\n",
    "    axes.set_ylim([0, 90])\n",
    "    axes.tick_params(labelleft=False)\n",
    "    axes.set_theta_zero_location(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2acebf-a0ad-441f-a9b9-420b9d83a923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot(dome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed33bf68-1e67-4538-b356-cdce9eecec79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_SVF(radius, dome):\n",
    "    obstructedArea = 0\n",
    "    treeObstruction = 0\n",
    "    buildObstruction = 0\n",
    "    for i in range(0, 180):\n",
    "        for j in range(0, 90):\n",
    "            if dome[i, j] != 0:\n",
    "                v = 90 - (j + 1)\n",
    "                R = math.cos(v * math.pi / 180) * radius\n",
    "                r = math.cos((v + 1) * math.pi / 180) * radius\n",
    "                # calculate area of each obstructed sector (circular sector area calculation)\n",
    "                cell_area = (math.pi / 180.0) * (R ** 2 - r ** 2)\n",
    "                obstructedArea += cell_area\n",
    "                if dome[i, j] in [5]:\n",
    "                    treeObstruction += cell_area\n",
    "                elif dome[i, j] == 6:\n",
    "                    buildObstruction += cell_area\n",
    "    circleArea = math.pi * (radius ** 2)\n",
    "    # SVF: proportion of open area to total area\n",
    "    SVF = (circleArea - obstructedArea) / circleArea\n",
    "    treeObstructionPercentage = treeObstruction / circleArea\n",
    "    buildObstructionPercentage = buildObstruction / circleArea\n",
    "    return SVF, treeObstructionPercentage, buildObstructionPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0480e8f2-a4ed-4de4-8fc9-ea3b4a4111f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from builtins import round  # For Python's built-in round\n",
    "SVF, tree_percentage, build_percentage = calculate_SVF(dome_radius, dome)\n",
    "SVF = round(SVF * 100,1)\n",
    "tree_percentage = round(tree_percentage * 100,1)\n",
    "build_percentage = round(build_percentage * 100,1)\n",
    "\n",
    "print('Sky: {}%'.format(int(SVF)) + \"\\n\" +\n",
    "      'Vegetation {}%'.format(int(tree_percentage)) + \"\\n\" +\n",
    "      'Building {}%'.format(int(build_percentage)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fdde48-b190-42bb-984f-660a07168a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT sum(z)/count(z)\n",
    "FROM result\n",
    "WHERE classification = frequent_class AND ST_DWITHIN(g_geometry, p_geometry)\n",
    "GROUP BY fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe3238d-4a8c-4cf5-a24d-e1864d2ac345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getheight(pc_df, x, y):\n",
    "    radius = 1.0  # meters\n",
    "    center_x = float(x)\n",
    "    center_y = float(y)\n",
    "\n",
    "    # Compute distance from (x, y) and add as column\n",
    "    pc_df = pc_df.withColumn(\n",
    "        \"distance\", sqrt(pow(col(\"X\") - center_x, 2) + pow(col(\"Y\") - center_y, 2))\n",
    "    )\n",
    "\n",
    "    # Filter ground and building points within radius\n",
    "    ground_points = pc_df.filter((col(\"Classification\") == 2) & (col(\"distance\") <= radius))\n",
    "    build_points = pc_df.filter((col(\"Classification\") == 6) & (col(\"distance\") <= radius))\n",
    "\n",
    "    # Count and aggregate heights\n",
    "    ground_count = ground_points.count()\n",
    "    build_count = build_points.count()\n",
    "\n",
    "    if ground_count > build_count:\n",
    "        selected_points = ground_points\n",
    "        count = ground_count\n",
    "    else:\n",
    "        selected_points = build_points\n",
    "        count = build_count\n",
    "\n",
    "    if count > 0:\n",
    "        total_height = selected_points.selectExpr(\"sum(Z) as total_z\").collect()[0][\"total_z\"]\n",
    "        return float(total_height) / count\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "025f31e5-629d-4fd8-948e-af11a8752094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getheight(tile_grid, x, y):\n",
    "    center = np.array([x, y])\n",
    "    pointheight = 0\n",
    "    points_number = 0\n",
    "\n",
    "    # Stream file to memory\n",
    "    for tile in tile_grid:\n",
    "        # read the .las file\n",
    "        file_input = read(las_data)\n",
    "        # keep groundpoints satisfying ground_rules:\n",
    "        # classification 2 for ground, inside las file\n",
    "        # keep points within radius of 5 metres\n",
    "        ground_rules = np.logical_and(\n",
    "        file_input.classification == 2,np.sqrt(np.sum((np.vstack((file_input.x,\n",
    "        file_input.y)).transpose() - center) ** 2, axis=1)) <= 1)\n",
    "        build_rules = np.logical_and(\n",
    "        file_input.classification == 6,\n",
    "        np.sqrt(np.sum((np.vstack((file_input.x,\n",
    "        file_input.y)).transpose() - center) ** 2, axis=1)) <= 1)\n",
    "\n",
    "        ground_points = file_input.points[ground_rules]\n",
    "        build_points = file_input.points[build_rules]\n",
    "        print(build_points)\n",
    "        # make array with heights of each point\n",
    "        if len(ground_points) > len(build_points):\n",
    "            ground_point_heights = np.array((ground_points.z)).transpose()\n",
    "        else:\n",
    "            ground_point_heights = np.array((build_points.z)).transpose()\n",
    "\n",
    "        if len(ground_point_heights) > 0:\n",
    "            pointheight += float(np.sum(ground_point_heights))\n",
    "            points_number += len(ground_point_heights)\n",
    "\n",
    "    # get mean value of points' heights\n",
    "    if points_number > 0:\n",
    "        height = pointheight / points_number\n",
    "        return height\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5dbd564-1bdf-4795-b26c-4a942a850801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def get_heights_from_df(pc_df, coord_df, radius=1.0):\n",
    "    \"\"\"\n",
    "    Optimized version: Filters classification FIRST, then calculates distances.\n",
    "    \"\"\"\n",
    "    # Broadcast coordinates (if small enough)\n",
    "    coord_bc = F.broadcast(coord_df.select(\"fid\", \"x1\", \"y1\"))\n",
    "\n",
    "    # Step 1: First filter by classification to reduce data\n",
    "    filtered_pc = pc_df.filter(F.col(\"classification\").isin([2, 6]))\n",
    "\n",
    "    # Step 2: Cross-join with coordinates and compute distances only for filtered points\n",
    "    joined_df = filtered_pc.crossJoin(coord_bc).withColumn(\n",
    "        \"distance\",\n",
    "        F.sqrt((F.col(\"X\") - F.col(\"x1\"))**2 + (F.col(\"Y\") - F.col(\"y1\"))**2)\n",
    "    ).filter(F.col(\"distance\") <= radius)  # Only need distance filter now\n",
    "\n",
    "    # Group and aggregate (same as before)\n",
    "    result_df = joined_df.groupBy(\"fid\").agg(\n",
    "        F.mean(\"Z\").alias(\"height\"),\n",
    "        F.count(\"*\").alias(\"point_count\")\n",
    "    )\n",
    "\n",
    "    # Join back with original coordinates\n",
    "    return  coord_df.join(\n",
    "        result_df.select(\"fid\", \"height\"),\n",
    "        on=\"fid\",\n",
    "        how=\"left\"\n",
    "    ).fillna(0.0, subset=[\"height\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292600de-99e2-410f-a400-cfb2f6f2ff26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_df_height = get_heights_from_df(df_selected, df_grid, 100)\n",
    "display(grid_df_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03737332-340b-4d20-a235-5160e34416dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Define a Pandas UDF to process partitions\n",
    "@pandas_udf(\"array<double>\")  # Return type: Spark array\n",
    "def to_numpy_udf(col: pd.Series) -> pd.Series:\n",
    "    # Each partition is converted to a list of NumPy arrays\n",
    "    return col.apply(lambda x: np.array(x).flatten())  # Ensure shape (n,)\n",
    "\n",
    "# Apply UDF and aggregate (if needed)\n",
    "np_arrays = df_selected.select(to_numpy_udf(\"X\").alias(\"np_array\"))\n",
    "\n",
    "# Collect results (if aggregation was done in UDF)\n",
    "final_array = np.concatenate(np_arrays.collect())\n",
    "print(final_array.shape)  # (n_points,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c62bbb8-1c78-4919-8731-c5e0f4430438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import FloatType  # or whatever your output type is\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "broadcast_tile_grid = spark.sparkContext.broadcast(tile_grid)\n",
    "tile_grid = broadcast_tile_grid.value\n",
    "\n",
    "\n",
    "\n",
    "# @pandas_udf(FloatType())\n",
    "# def getheight_udf(x_series: pd.Series, y_series: pd.Series) -> pd.Series:\n",
    "#     # Get the broadcasted tile grid\n",
    "#     # tile_grid = broadcast_tile_grid.value\n",
    "\n",
    "#     # print(tile_grid)\n",
    "    \n",
    "#     results = []\n",
    "#     # for x, y in zip(x_series, y_series):\n",
    "#     #     results.append(getheight(x, y, tile_grid))\n",
    "#     return pd.Series(results)\n",
    "\n",
    "\n",
    "@pandas_udf(FloatType())\n",
    "def getheight_udf(x_series: pd.Series, y_series: pd.Series) -> pd.Series:\n",
    "    results = []\n",
    "    for x, y in zip(x_series, y_series):\n",
    "        results.append(getheight(x, y, tile_grid))\n",
    "    return pd.Series(results)\n",
    "\n",
    "# Apply the UDF\n",
    "df_with_height = df.withColumn(\"height\", getheight_udf(df[\"x\"], df[\"y\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6631496815146743,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "09-pointclouds-pdal-pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
